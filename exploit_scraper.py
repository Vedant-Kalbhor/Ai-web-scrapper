import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
import json

def get_exploit_sources():
    """
    Returns list of reliable exploit sources to scrape.
    """
    return [
        {
            "url": "https://www.exploit-db.com/",
            "type": "api",  # Changed to API
            "name": "Exploit Database"
        },
        {
            "url": "https://packetstormsecurity.com/files/tags/exploit/",
            "type": "html",
            "name": "Packet Storm Security"
        },
        {
            "url": "https://cxsecurity.com/exploit/",
            "type": "html",
            "name": "CXSecurity"
        }
    ]

def scrape_exploitdb_api(limit=10):
    """
    Scrape Exploit-DB using their internal API (MUCH MORE RELIABLE!)
    This is the same API their website uses.
    """
    exploits = []
    
    try:
        print(f"  ‚Üí Fetching exploits from Exploit-DB API")
        
        # Step 1: Get session cookies
        headers = {
            'Host': 'www.exploit-db.com',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.71 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive'
        }
        
        response = requests.get("https://exploit-db.com/", headers=headers, timeout=15)
        
        # Extract cookies
        xsrf_token = response.headers["Set-Cookie"].split("XSRF-TOKEN=")[1].split(';')[0]
        exploit_database_session = response.headers["Set-Cookie"].split("exploit_database_session=")[1].split(';')[0]
        
        # Update headers for API requests
        headers["Cookie"] = f"XSRF-TOKEN={xsrf_token}; exploit_database_session={exploit_database_session}"
        headers["Accept"] = "application/json, text/javascript, */*; q=0.01"
        headers["X-Requested-With"] = "XMLHttpRequest"
        headers["Referer"] = "https://www.exploit-db.com/"
        
        print(f"    ‚úì Got session cookies")
        
        # Step 2: Fetch exploits via API
        draw = 1
        length = 10 # Exploits per request
        total_fetched = 0
        max_requests = max(1, limit // length + 1)
        
        print(f"    ‚Üí Fetching {limit} exploits (this may take 10-30 seconds)...")
        
        for request_num in range(1, max_requests + 1):
            try:
                start = (draw - 1) * length
                
                # Build API URL (same as website uses)
                api_url = f"https://exploit-db.com/?draw={draw}&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start={start}&length={length}&search%5Bvalue%5D=&author=&port=&type=&tag=&platform="
                
                response = requests.get(api_url, headers=headers, timeout=20)
                
                # Update cookies for next request
                if "Set-Cookie" in response.headers:
                    xsrf_token = response.headers["Set-Cookie"].split("XSRF-TOKEN=")[1].split(';')[0]
                    exploit_database_session = response.headers["Set-Cookie"].split("exploit_database_session=")[1].split(';')[0]
                    headers["Cookie"] = f"XSRF-TOKEN={xsrf_token}; exploit_database_session={exploit_database_session}"
                
                json_response = json.loads(response.text)
                data = json_response.get("data", [])
                
                if not data:
                    print(f"    ‚ö† No more data available")
                    break
                
                # Process each exploit
                for exploit in data:
                    try:
                        # Extract and format data
                        exploit_id = exploit.get("id", "")
                        description = exploit.get("description", ["", "Unknown"])
                        title = description[1] if isinstance(description, list) and len(description) > 1 else str(description)
                        
                        # Get platform
                        platform = exploit.get("platform_id", "Unknown")
                        if isinstance(platform, list):
                            platform = platform[1] if len(platform) > 1 else str(platform[0])
                        
                        # Get author
                        author = exploit.get("author_id", "Unknown")
                        if isinstance(author, list):
                            author = author[1] if len(author) > 1 else str(author[0])
                        
                        # Get type
                        exploit_type = exploit.get("type_id", "Unknown")
                        if isinstance(exploit_type, list):
                            exploit_type = exploit_type[1] if len(exploit_type) > 1 else str(exploit_type[0])
                        
                        # Get date
                        date_published = exploit.get("date_published", datetime.now().strftime('%Y-%m-%d'))
                        
                        # Check if verified
                        verified = exploit.get("verified", 0) == 1
                        
                        formatted_exploit = {
                            'id': f"EDB-{exploit_id}",
                            'title': title[:200],  # Limit title length
                            'platform': str(platform),
                            'author': str(author)[:50],
                            'type': str(exploit_type),
                            'date': date_published,
                            'verified': verified,
                            'source': 'Exploit-DB',
                            'url': f"https://www.exploit-db.com/exploits/{exploit_id}"
                        }
                        
                        exploits.append(formatted_exploit)
                        total_fetched += 1
                        
                        if total_fetched >= limit:
                            break
                        
                    except Exception as e:
                        print(f"    ‚ö† Error parsing exploit: {e}")
                        continue
                
                print(f"    ‚úì Request {request_num}: {len(data)} exploits (Total: {total_fetched})")
                
                if total_fetched >= limit:
                    break
                
                draw += 1
                time.sleep(0.5)  # Rate limiting
                
            except Exception as e:
                print(f"    ‚úó Error on request {request_num}: {e}")
                break
        
        print(f"  ‚úì Exploit-DB API complete: {len(exploits)} exploits")
        return exploits
        
    except Exception as e:
        print(f"  ‚úó Exploit-DB API critical error: {e}")
        import traceback
        traceback.print_exc()
        return exploits

def scrape_packetstorm():
    """
    Scrape latest exploits from Packet Storm Security
    """
    exploits = []
    
    try:
        print(f"  ‚Üí Fetching exploits from Packet Storm")
        session = requests.Session()
        
        for page in range(1, 4):  # 3 pages
            try:
                url = f"https://packetstormsecurity.com/files/tags/exploit/page{page}/" if page > 1 else "https://packetstormsecurity.com/files/tags/exploit/"
                
                print(f"    ‚Üí Fetching page {page}...")
                response = session.get(
                    url,
                    timeout=20,
                    headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                    }
                )
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find exploit entries
                items = soup.select('dl.file')
                
                print(f"    ‚Üí Found {len(items)} items on page {page}")
                
                page_exploits = 0
                for idx, item in enumerate(items):
                    try:
                        title_elem = item.find('dt')
                        if not title_elem:
                            continue
                        
                        link = title_elem.find('a')
                        if not link:
                            continue
                        
                        title = link.get_text(strip=True)
                        file_url = link.get('href', '')
                        file_id = file_url.split('/')[-1] if file_url else f"unknown-{idx}"
                        
                        # Get description
                        desc_elem = item.find('dd', class_='detail')
                        description = desc_elem.get_text(strip=True) if desc_elem else ""
                        
                        # Extract date
                        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', description)
                        date = date_match.group(1) if date_match else datetime.now().strftime('%Y-%m-%d')
                        
                        # Extract author
                        author_match = re.search(r'by ([^\n]+)', description)
                        author = author_match.group(1).strip() if author_match else "Unknown"
                        
                        exploit = {
                            'id': f"PS-{file_id}",
                            'title': title,
                            'platform': 'Multiple',
                            'author': author[:50],
                            'type': 'Exploit',
                            'date': date,
                            'verified': False,
                            'source': 'Packet Storm',
                            'url': f"https://packetstormsecurity.com{file_url}" if file_url.startswith('/') else file_url
                        }
                        
                        exploits.append(exploit)
                        page_exploits += 1
                        
                    except Exception as e:
                        continue
                
                print(f"    ‚úì Page {page}: Extracted {page_exploits} exploits (Total: {len(exploits)})")
                time.sleep(1)
                
            except Exception as e:
                print(f"    ‚úó Error on page {page}: {e}")
                continue
        
        session.close()
        print(f"  ‚úì Packet Storm complete: {len(exploits)} exploits")
        return exploits
        
    except Exception as e:
        print(f"  ‚úó Packet Storm critical error: {e}")
        return exploits

def scrape_cxsecurity():
    """
    Scrape latest exploits from CXSecurity
    """
    exploits = []
    
    try:
        print(f"  ‚Üí Fetching exploits from CXSecurity")
        session = requests.Session()
        
        for page in range(1, 4):
            try:
                url = f"https://cxsecurity.com/exploit/{page}/" if page > 1 else "https://cxsecurity.com/exploit/"
                
                print(f"    ‚Üí Fetching page {page}...")
                response = session.get(
                    url,
                    timeout=20,
                    headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                )
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                rows = soup.find_all('tr')
                
                page_exploits = 0
                for idx, row in enumerate(rows[1:]):
                    try:
                        cells = row.find_all('td')
                        if len(cells) < 3:
                            continue
                        
                        title_elem = None
                        for cell in cells:
                            link = cell.find('a')
                            if link and link.get_text(strip=True):
                                title_elem = link
                                break
                        
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        exploit_url = title_elem.get('href', '')
                        exploit_id = exploit_url.split('/')[-1] if exploit_url else f"unknown-{idx}"
                        
                        date_text = cells[0].get_text(strip=True)
                        date_match = re.search(r'\d{4}-\d{2}-\d{2}', date_text)
                        date = date_match.group(0) if date_match else datetime.now().strftime('%Y-%m-%d')
                        
                        author = cells[-1].get_text(strip=True) if len(cells) > 2 else "Unknown"
                        
                        exploit = {
                            'id': f"CX-{exploit_id}",
                            'title': title,
                            'platform': 'Multiple',
                            'author': author[:50],
                            'type': 'Exploit',
                            'date': date,
                            'verified': False,
                            'source': 'CXSecurity',
                            'url': f"https://cxsecurity.com{exploit_url}" if exploit_url.startswith('/') else exploit_url
                        }
                        
                        exploits.append(exploit)
                        page_exploits += 1
                        
                    except Exception as e:
                        continue
                
                print(f"    ‚úì Page {page}: Extracted {page_exploits} exploits (Total: {len(exploits)})")
                time.sleep(1)
                
            except Exception as e:
                print(f"    ‚úó Error on page {page}: {e}")
                continue
        
        session.close()
        print(f"  ‚úì CXSecurity complete: {len(exploits)} exploits")
        return exploits
        
    except Exception as e:
        print(f"  ‚úó CXSecurity critical error: {e}")
        return exploits

def scrape_exploit_source(source_dict):
    """
    Smart scraper that routes to appropriate function
    """
    name = source_dict.get("name", "")
    source_type = source_dict.get("type", "html")
    
    print(f"\nüîç Scraping: {name}")
    
    try:
        if "Exploit Database" in name and source_type == "api":
            return scrape_exploitdb_api(limit=10)
        # elif "Exploit Database" in name:
        #     return scrape_exploitdb_api(limit=10)
        # elif "Packet Storm" in name:
        #     return scrape_packetstorm()
        # elif "CXSecurity" in name:
        #     return scrape_cxsecurity()
        else:
            print(f"  ‚ö† Unknown source: {name}")
            return []
    except Exception as e:
        print(f"  ‚úó Failed to scrape {name}: {e}")
        import traceback
        traceback.print_exc()
        return []

def scrape_all_exploits_parallel(max_workers=2):
    """
    Scrape all exploit sources in parallel
    """
    sources = get_exploit_sources()
    all_exploits = []

    print(f"\nüöÄ Starting parallel exploit scraping from {len(sources)} sources...")
    print(f"‚è±Ô∏è  This may take 20-40 seconds...\n")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_source = {
            executor.submit(scrape_exploit_source, source): source 
            for source in sources
        }

        for future in as_completed(future_to_source):
            source = future_to_source[future]
            try:
                exploits = future.result(timeout=90)
                if exploits:
                    print(f"  ‚úÖ {source.get('name')}: {len(exploits)} exploits")
                    all_exploits.extend(exploits)
                else:
                    print(f"  ‚ö†Ô∏è  {source.get('name')}: No exploits found")
            except Exception as e:
                print(f"  ‚ùå Error processing {source.get('name')}: {e}")

    print(f"\nüìä Raw total: {len(all_exploits)} exploits before deduplication")

    # Remove duplicates based on normalized title
    seen_titles = set()
    unique_exploits = []
    
    for exploit in all_exploits:
        # Normalize title for comparison
        title_normalized = re.sub(r'[^\w\s]', '', exploit['title'].lower()).strip()
        unique_key = f"{title_normalized}_{exploit.get('platform', 'unknown').lower()}"
        
        if unique_key not in seen_titles:
            seen_titles.add(unique_key)
            unique_exploits.append(exploit)

    # Sort by date (newest first)
    unique_exploits.sort(key=lambda x: x.get('date', '1970-01-01'), reverse=True)

    print(f"\n‚úÖ Parallel exploit scraping complete!")
    print(f"üì¶ Total unique exploits: {len(unique_exploits)}")
    print(f"üóëÔ∏è  Duplicates removed: {len(all_exploits) - len(unique_exploits)}\n")
    
    # Show sample
    if unique_exploits:
        print("üìã Sample exploits:")
        for exploit in unique_exploits[:5]:
            verified_mark = "‚úì" if exploit.get('verified') else ""
            print(f"  - [{exploit['id']}] {verified_mark} {exploit['title'][:70]}")
    
    executor.shutdown(wait=True, cancel_futures=True)
    return unique_exploits


# Test function
if __name__ == "__main__":
    print("üß™ Testing exploit scraper with API method...\n")
    exploits = scrape_all_exploits_parallel()
    
    if exploits:
        print(f"\n‚úÖ SUCCESS: Found {len(exploits)} exploits")
        
        # Show breakdown by source
        from collections import Counter
        source_counts = Counter(e['source'] for e in exploits)
        print("\nüìä Breakdown by source:")
        for source, count in source_counts.items():
            print(f"  - {source}: {count} exploits")
        
        # Show breakdown by type
        type_counts = Counter(e.get('type', 'Unknown') for e in exploits)
        print("\nüìä Breakdown by type:")
        for exploit_type, count in type_counts.items():
            print(f"  - {exploit_type}: {count} exploits")
    else:
        print("\n‚ùå FAILED: No exploits found")